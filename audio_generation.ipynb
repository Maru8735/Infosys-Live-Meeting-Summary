{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maru8735/Infosys-Live-Meeting-Summary/blob/main/audio_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy soundfile\n"
      ],
      "metadata": {
        "id": "GIUaOX-g7m4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# ====== SETTINGS ======\n",
        "duration = 3        # seconds\n",
        "sample_rate = 16000 # 16 kHz audio\n",
        "frequency = 440     # A4 tone (can change)\n",
        "\n",
        "# ====== GENERATE SINE WAVE ======\n",
        "t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
        "audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
        "\n",
        "# ====== SAVE AUDIO FILE ======\n",
        "output_file = \"generated_audio.wav\"\n",
        "sf.write(output_file, audio_data, sample_rate)\n",
        "\n",
        "# ====== PRINT OUTPUT ======\n",
        "print(\"Audio file created successfully!\")\n",
        "print(\"Saved as:\", output_file)\n",
        "\n",
        "# ====== PLAY AUDIO IN COLAB ======\n",
        "display(Audio(output_file, autoplay=True))\n"
      ],
      "metadata": {
        "id": "7JlOqGWF7qba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do5-nl0BGNJr"
      },
      "outputs": [],
      "source": [
        "!pip install soundfile mutagen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlFpPM3EGY3s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "from mutagen.wave import WAVE\n",
        "from mutagen.id3 import TextFrame\n",
        "\n",
        "# ====== SETTINGS ======\n",
        "duration = 3        # seconds\n",
        "sample_rate = 16000 # 16 kHz audio\n",
        "frequency = 440     # Sine tone frequency\n",
        "\n",
        "# ====== GENERATE SINE WAVE ======\n",
        "t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
        "audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
        "\n",
        "# ====== SAVE AUDIO FILE ======\n",
        "output_file = \"generated_audio_with_info.wav\"\n",
        "sf.write(output_file, audio_data, sample_rate)\n",
        "\n",
        "# ====== ADD METADATA ======\n",
        "metadata = WAVE(output_file)\n",
        "\n",
        "# Use TextFrame for metadata values\n",
        "metadata[\"INAM\"] = TextFrame(encoding=3, text=[\"Sample Tone Audio\"])# title or name of the audio\n",
        "metadata[\"IART\"] = TextFrame(encoding=3, text=[\"Your Name\"]) # artist\n",
        "metadata[\"ICMT\"] = TextFrame(encoding=3, text=[\"This is a generated audio tone with embedded metadata.\"]) #comments\n",
        "metadata[\"ICRD\"] = TextFrame(encoding=3, text=[\"2025-02-14\"]) # creation date\n",
        "metadata.save()\n",
        "\n",
        "# ====== PLAY AUDIO ======\n",
        "print(\"Audio file created with metadata!\\n\")\n",
        "display(Audio(output_file, autoplay=False))\n",
        "\n",
        "# ====== READ AND PRINT METADATA ======\n",
        "print(\"=== Embedded Metadata in WAV File ===\")\n",
        "for key, value in metadata.items():\n",
        "    print(f\"{key}: {value.text[0] if hasattr(value, 'text') else value}\") # Access text attribute for TextFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyklHTMxGerV"
      },
      "outputs": [],
      "source": [
        "!pip install gTTS soundfile # google text to speech\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lrmo4Z2Giam"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio, display\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "# ====== TEXT TO SPEECH INPUT ======\n",
        "text = \"Hello! This is an automatically generated audio message created in Google Colab.\"\n",
        "\n",
        "# ====== GENERATE AUDIO FROM TEXT ======\n",
        "tts = gTTS(text=text, lang='en')\n",
        "tts.save(\"text_audio.mp3\")\n",
        "\n",
        "# Convert MP3 to WAV (optional)\n",
        "# Load MP3 using audio libraries\n",
        "import librosa\n",
        "audio_data, sr = librosa.load(\"text_audio.mp3\", sr=16000)\n",
        "sf.write(\"text_audio.wav\", audio_data, sr)\n",
        "\n",
        "# ====== PLAY AUDIO ======\n",
        "print(\"Text converted to speech and saved as 'text_audio.wav'\")\n",
        "display(Audio(\"text_audio.wav\", autoplay=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR9qN9a4Gm_Q"
      },
      "outputs": [],
      "source": [
        "!pip install sounddevice vosk faster-whisper soundfile\n",
        "!apt-get install -y portaudio19-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSIIicucG2M7"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "\n",
        "tts = gTTS(\"Hello, this is a test audio for speech to text models.\", lang=\"en\")\n",
        "tts.save(\"test_audio.mp3\")\n",
        "\n",
        "Audio(\"test_audio.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVB2KI-WHcGc"
      },
      "source": [
        "convert to WAV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QMv2a6wG_m_"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -i test_audio.mp3 -ar 16000 -ac 1 test.wav -y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3QrMrouHkXo"
      },
      "source": [
        "Run in vosk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucwDB79xHX9C"
      },
      "outputs": [],
      "source": [
        "!wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
        "!unzip -q vosk-model-small-en-us-0.15.zip\n",
        "\n",
        "from vosk import Model, KaldiRecognizer\n",
        "import wave, json\n",
        "\n",
        "wf = wave.open(\"test.wav\", \"rb\")\n",
        "rec = KaldiRecognizer(Model(\"vosk-model-small-en-us-0.15\"), 16000)\n",
        "\n",
        "result = \"\"\n",
        "while True:\n",
        "    data = wf.readframes(4000)\n",
        "    if not data:\n",
        "        break\n",
        "    if rec.AcceptWaveform(data):\n",
        "        result += json.loads(rec.Result())[\"text\"] + \" \"\n",
        "\n",
        "result += json.loads(rec.FinalResult())[\"text\"]\n",
        "print(\"VOSK:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7u_2lftHsyW"
      },
      "source": [
        "RUN IN WHISPER\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faster-whisper\n",
        "\n",
        "from faster_whisper import WhisperModel\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = WhisperModel(\"small\", device=device)\n",
        "\n",
        "segments, info = model.transcribe(\"test.wav\")\n",
        "text = \" \".join([s.text for s in segments])\n",
        "\n",
        "print(\"WHISPER:\", text)\n"
      ],
      "metadata": {
        "id": "rNJUPqDt7JVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb0z6cTJHxs9"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Single-Cell STT Pipeline (Colab)\n",
        "# TTS → WAV → Vosk → Whisper\n",
        "# ================================\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q gTTS vosk faster-whisper soundfile sentencepiece\n",
        "!apt-get -qq install -y ffmpeg\n",
        "\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "import subprocess, wave, json, os\n",
        "import torch\n",
        "\n",
        "# 1) Generate Speech Audio using TTS\n",
        "text_input = \"Hello, this is an automatic speech recognition test using Vosk and Whisper.\"\n",
        "tts = gTTS(text_input, lang=\"en\")\n",
        "tts.save(\"tts.mp3\")\n",
        "print(\"Generated audio from text:\", text_input)\n",
        "Audio(\"tts.mp3\")\n",
        "\n",
        "# 2) Convert MP3 → WAV (16 kHz mono)\n",
        "subprocess.run([\"ffmpeg\", \"-y\", \"-i\", \"tts.mp3\", \"-ar\", \"16000\", \"-ac\", \"1\", \"audio.wav\"],\n",
        "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "print(\"Converted to audio.wav (16k mono)\")\n",
        "\n",
        "# 3) --- VOSK STT ---\n",
        "if not os.path.exists(\"vosk-model\"):\n",
        "    !wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip -O model.zip\n",
        "    !unzip -q model.zip\n",
        "    !mv vosk-model-small-en-us-0.15 vosk-model\n",
        "    !rm model.zip\n",
        "\n",
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "wf = wave.open(\"audio.wav\", \"rb\")\n",
        "rec = KaldiRecognizer(Model(\"vosk-model\"), 16000)\n",
        "\n",
        "vosk_text = \"\"\n",
        "while True:\n",
        "    data = wf.readframes(4000)\n",
        "    if not data: break\n",
        "    if rec.AcceptWaveform(data):\n",
        "        vosk_text += json.loads(rec.Result())[\"text\"] + \" \"\n",
        "vosk_text += json.loads(rec.FinalResult())[\"text\"]\n",
        "\n",
        "# 4) --- WHISPER STT ---\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "whisper = WhisperModel(\"small\", device=device)\n",
        "\n",
        "segments, _ = whisper.transcribe(\"audio.wav\")\n",
        "whisper_text = \" \".join([s.text for s in segments]).strip()\n",
        "\n",
        "# 5) Results\n",
        "print(\"\\n===== VOSK OUTPUT =====\")\n",
        "print(vosk_text)\n",
        "\n",
        "print(\"\\n===== WHISPER OUTPUT =====\")\n",
        "print(whisper_text)\n",
        "\n",
        "print(\"\\n===== ORIGINAL TEXT =====\")\n",
        "print(text_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PjGMEy6IAcD"
      },
      "source": [
        "DOWNLOAD AMI SAMPLE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6pHZlxwpH8Sr"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ami_sample\n",
        "!wget -q https://groups.inf.ed.ac.uk/ami/AMICorpusSamples/ES2002a.Mix-Headset.wav -O ami_sample/ES2002a.wav\n",
        "\n",
        "print(\"Downloaded AMI sample file:\")\n",
        "!ls -lh ami_sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umFTkrfNIG-p"
      },
      "source": [
        "Load the AMI sample audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIs9avI1IDjO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(\"ami_sample/ES2002a.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk4TXRV0IQFl"
      },
      "source": [
        "Download a public speech dataset (LibriSpeech test subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "360rh6PDIKE7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p librispeech_sample\n",
        "!wget -q https://www.openslr.org/resources/12/dev-clean.tar.gz -O dev-clean.tar.gz\n",
        "!tar -xzf dev-clean.tar.gz --directory librispeech_sample --wildcards \"*.flac\"\n",
        "\n",
        "print(\"Sample files:\")\n",
        "!find librispeech_sample -name \"*.flac\" | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0Xxg8EfIMym"
      },
      "outputs": [],
      "source": [
        "!mkdir -p librispeech_sample\n",
        "!wget -q https://www.openslr.org/resources/12/dev-clean.tar.gz -O dev-clean.tar.gz\n",
        "!tar -xzf dev-clean.tar.gz --directory librispeech_sample --wildcards \"*.flac\"\n",
        "\n",
        "print(\"Sample files:\")\n",
        "!find librispeech_sample -name \"*.flac\" | head\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN-SUWNKJZ8x"
      },
      "source": [
        "Convert dataset audio (FLAC → WAV, 16k mono)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYitGPGpJV15"
      },
      "outputs": [],
      "source": [
        "import subprocess, os\n",
        "\n",
        "source = \"librispeech_sample/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac\"\n",
        "target = \"sample.wav\"\n",
        "\n",
        "subprocess.run([\"ffmpeg\", \"-y\", \"-i\", source, \"-ar\", \"16000\", \"-ac\", \"1\", target])\n",
        "\n",
        "print(\"Converted to:\", target)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHgXlrDnJs87"
      },
      "source": [
        "Create a synthetic dataset automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Avvxb70JjFz"
      },
      "outputs": [],
      "source": [
        "!pip install gTTS soundfile\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "os.makedirs(\"synthetic_dataset\", exist_ok=True)\n",
        "\n",
        "sentences = [\n",
        "    \"Hello, welcome to the speech recognition test.\",\n",
        "    \"This is a synthetic dataset created using text to speech.\",\n",
        "    \"Speech models must be evaluated for accuracy.\",\n",
        "    \"Different speakers and accents should be tested.\",\n",
        "    \"Background noise can affect transcription quality.\",\n",
        "    \"We will benchmark whisper and vosk models.\",\n",
        "    \"This sentence is intentionally longer to test robustness.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is transforming industries.\",\n",
        "    \"Thank you for participating in this project.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(sentences):\n",
        "    tts = gTTS(text=text, lang=\"en\")\n",
        "    path = f\"synthetic_dataset/audio_{i}.mp3\"\n",
        "    tts.save(path)\n",
        "\n",
        "print(\"Synthetic dataset created:\")\n",
        "!ls -1 synthetic_dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conversion of files of a folder into audio"
      ],
      "metadata": {
        "id": "ejLC0oYrj9MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS soundfile\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "input_folder = \"text_files\"\n",
        "output_folder = \"audio_output\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "text_files = [f for f in os.listdir(input_folder) if f.endswith(\".txt\")]\n",
        "\n",
        "for fname in text_files:\n",
        "    file_path = os.path.join(input_folder, fname)\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().strip()\n",
        "\n",
        "    if len(text) == 0:\n",
        "        print(f\"Skipping empty file: {fname}\")\n",
        "        continue\n",
        "\n",
        "    tts = gTTS(text=text, lang=\"en\")\n",
        "\n",
        "    base_name = os.path.splitext(fname)[0]\n",
        "    output_path = os.path.join(output_folder, base_name + \".mp3\")\n",
        "\n",
        "    tts.save(output_path)\n",
        "    print(\"Created:\", output_path)\n",
        "\n",
        "print(\"\\n All text files converted to audio!\")\n"
      ],
      "metadata": {
        "id": "9OTdmupDj6Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgx8sWFbJ15v"
      },
      "source": [
        "Convert synthetic audios to WAV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8c9re8TJqyR"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "files = os.listdir(\"synthetic_dataset\")\n",
        "\n",
        "for f in files:\n",
        "    if f.endswith(\".mp3\"):\n",
        "        mp3_path = f\"synthetic_dataset/{f}\"\n",
        "        wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
        "        subprocess.run([\"ffmpeg\", \"-y\", \"-i\", mp3_path, \"-ar\", \"16000\", \"-ac\", \"1\", wav_path])\n",
        "\n",
        "print(\"Converted WAV files:\")\n",
        "!ls synthetic_dataset/*.wav\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBdWo1oIXJYl"
      },
      "source": [
        "this the code for how to convert lontg text to audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_QPH685J4v0"
      },
      "outputs": [],
      "source": [
        "!pip install gTTS\n",
        "from gtts import gTTS\n",
        "text=open(\"text.txt\").read()\n",
        "tts=gTTS(text=text,lang=\"en\")\n",
        "tts.save(\"speech.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StS6UYWeYpwD"
      },
      "outputs": [],
      "source": [
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecA87PHOagfv"
      },
      "outputs": [],
      "source": [
        "from TTS.api import TTS\n",
        "\n",
        "#load a male english model\n",
        "tts=TTS(\"tts_models/en/ljspeech/glow-tts\").to(\"cpu\")\n",
        "text=\"Hello, this is an example of a male voice.\"\n",
        "tts.tts_to_file(text=text, file_path=\"male_voice.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcprQ3fhbfQG"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7uMYF-ocK2r"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}